{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souvikmaity797/Breast-Cancer-Prediction/blob/main/LSTM_nxtwordpred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "import random\n",
        "import sys"
      ],
      "metadata": {
        "id": "rLEocesvhnne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path = keras.utils.get_file(\n",
        "    'nietzsche.txt',\n",
        "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "text = open(path).read().lower()\n",
        "print('Text length:', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bskrof4whnvD",
        "outputId": "fb1c6775-01bb-4739-f8fa-f5ea34d30b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "\u001b[1m600901/600901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Text length: 600893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 60\n",
        "step = 3\n",
        "sentences = []\n",
        "next_char = []\n",
        "for i in range(0, len(text) - max_len, step):\n",
        "    sentences.append(text[i:i + max_len])\n",
        "    next_char.append(text[i + max_len])\n",
        "\n",
        "print('Number of sequences:', len(sentences), '   ',len(next_char))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3gNWF5phnzG",
        "outputId": "b2fbc8a4-46cf-4be0-84aa-48d7e1ab95bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 200278     200278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_chars = sorted(list(set(text)))\n",
        "print('Unique characters:', len(unique_chars))\n",
        "\n",
        "char_indices = dict((char,unique_chars.index(char)) for char in unique_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4MH3NmChn2I",
        "outputId": "ed81a65c-87e4-4e74-fc87-76ae0ccbc590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters: 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((len(sentences), max_len, len(unique_chars)), dtype=bool)\n",
        "y = np.zeros((len(next_char), len(unique_chars)), dtype=bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_char[i]]] = 1"
      ],
      "metadata": {
        "id": "8TiaEhQDhn5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(max_len, len(unique_chars))))\n",
        "model.add(layers.Dense(len(unique_chars), activation='softmax'))\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1JnFvewhn_P",
        "outputId": "6f04ea60-68ae-49bd-ec6b-a354e56bb81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getsample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probability = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probability)\n"
      ],
      "metadata": {
        "id": "espXR613hoCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "for epoch in range(0, 1):\n",
        "    print('Epoch', epoch)\n",
        "    model.fit(x, y, batch_size=128, epochs=1)"
      ],
      "metadata": {
        "id": "epOIOiSBmnRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039ee226-477f-436f-d263-ee004b0e59cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "\u001b[1m1565/1565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 196ms/step - loss: 2.3161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_index = random.randint(0, len(text) - max_len - 1)\n",
        "seed_text = text[start_index:start_index + max_len]"
      ],
      "metadata": {
        "id": "KIwx_ox-nhuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for temperature in (0.2, 0.5, 1.0):\n",
        "    print('temperature: ', temperature)\n",
        "    sys.stdout.write(seed_text)\n",
        "    generated_text = seed_text\n",
        "    for i in range(400):\n",
        "        sampled = np.zeros((1, max_len, len(unique_chars)))\n",
        "        for t, char in enumerate(generated_text):\n",
        "           sampled[0, t, char_indices[char]] = 1.\n",
        "        preds = model.predict(sampled, verbose=0)[0]\n",
        "        next_index = getsample(preds, temperature)\n",
        "        next_generated_char = unique_chars[next_index]\n",
        "        generated_text += next_generated_char\n",
        "        generated_text = generated_text[1:]\n",
        "\n",
        "        sys.stdout.write(next_generated_char)\n",
        "        sys.stdout.flush()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6NRc7mgnh4F",
        "outputId": "38a42c55-d1c9-4274-8933-210bd61d308d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temperature:  0.2\n",
            "ore correctly mislead) to his \"categorical\n",
            "imperative\"--make the for the such and the soul the befine to be and the such and the strangers and the soul so the such and the seater to the soull the such and which the mode and the such the such a man in the such and such a mode of the serious the such and and the soul has the strangers and the soul and has the fain of the consciently and the streathered and the more the man and such a possible to and the scoo\n",
            "temperature:  0.5\n",
            "ore correctly mislead) to his \"categorical\n",
            "imperative\"--make not and such of the same the possess of the for a midder and in the conscuctered of a langer and be dond have and be in the mode and readod of the\n",
            "soul, so possion of the thoughts and conscious there more the from the bad and men and be a reads to the for the which such a philosophers to the beatared of the our to his light and can and will differently dangers and be has the las and the been cons\n",
            "temperature:  1.0\n",
            "ore correctly mislead) to his \"categorical\n",
            "imperative\"--make will suffer, had spation was that it fee thorly ref a hampo-ghing-me hancauspes, streathurigy be what the slort?\n",
            "\n",
            "\n",
            "2\n",
            "3. thenargly to be every from the vertepally sm demread\n",
            "of dee camrly sas his reed, to un enjughly have reaknailx.f--that mea the ere an agpoctionfund\n",
            "befline of land what\n",
            "hustain. the god this by noed no concecterly so deedalrains readhed; and resdindt, with the hamb\", nowliess an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbUbyeX9nh7j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}